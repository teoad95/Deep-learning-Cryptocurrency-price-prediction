{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "hypertuning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6vbbPZXa4NA"
      },
      "source": [
        "##Data preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxp5YOq5a6W9"
      },
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard\n",
        "\n",
        "import tensorflow as tf\n",
        "import datetime"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WU3xS8sWa4NC"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import datetime"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nL8UN5Wva4ND"
      },
      "source": [
        "Define the path to load all locally stored csv with data from yahoo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsvatvGda4NE"
      },
      "source": [
        "path = os.getcwd()+\"/sample_data/Datasets/\" #Define where are the datasets"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJIScQk3a4NE"
      },
      "source": [
        "Create the whole dataset: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYrfSm-fa4NF"
      },
      "source": [
        "def dataset_creation(crypto_list, pth):\n",
        "    from google.colab import files\n",
        "    import io\n",
        "    \n",
        "    datasets = {} #Store all datasets here\n",
        "    dates = {} #Store all minimum dates here\n",
        "    \n",
        "    datasets_list = uploaded = files.upload()#os.listdir(pth) #Find all dataset\n",
        "    datasets_list = [key for key in uploaded.keys()]\n",
        "    \n",
        "    crypto_list = [elem+'.csv' for elem in crypto_list]\n",
        "\n",
        "    for dataset in datasets_list:\n",
        "        if dataset.endswith(\".csv\") and (dataset) in crypto_list:\n",
        "\n",
        "            name = dataset.split(\".\")\n",
        "            dataset_name = name[0]\n",
        "\n",
        "            datasets[dataset_name] = pd.read_csv(io.StringIO(uploaded[dataset].decode('utf-8'))) #pd.read_csv(path + dataset) \n",
        "\n",
        "            datasets[dataset_name]['Date'] = pd.to_datetime(datasets[dataset_name]['Date'])\n",
        "            datasets[dataset_name].fillna(method='ffill', inplace=True)\n",
        "\n",
        "            #Create new columns 'close_off_high' and 'volatility' in order to make predictions more accurate:\n",
        "            kwards = {'close_off_high': lambda x: 2 * (x['High'] - x['Close']) / (x['High'] - x['Low']) - 1,\n",
        "              'volatility': lambda x: (x['High'] - x['Low']) / (x['Open'])\n",
        "              }\n",
        "\n",
        "            datasets[dataset_name] = datasets[dataset_name].assign(**kwards)\n",
        "\n",
        "            first_date = pd.to_datetime(datasets[dataset_name]['Date'][0])\n",
        "            dates[dataset_name]=first_date\n",
        "\n",
        "    max_date = max(dates.values(), key=lambda v: v)\n",
        "\n",
        "    #Drop all the data which are prior to max_date\n",
        "    for dataset in datasets:\n",
        "        datasets[dataset] = datasets[dataset][datasets[dataset]['Date'] >= max_date]\n",
        "\n",
        "\n",
        "    #Compute the average and standard deviation of 'Close' value for the last 7-days and 30-days(month): \n",
        "    for dataset in datasets:\n",
        "\n",
        "        temp = datasets[dataset].copy()\n",
        "\n",
        "        #Drop the first 30 days to be able to compute average and standard deviation of month for the rows of the table\n",
        "        temp = temp[29:]\n",
        "\n",
        "        temp['mean_7days_Close'] = datasets[dataset]['Close'].rolling(window=7).mean()\n",
        "\n",
        "        temp['mean_month_Close'] = datasets[dataset]['Close'].rolling(window=30).mean()\n",
        "\n",
        "        temp['std_7days_Close'] = datasets[dataset]['Close'].rolling(window=7).std()\n",
        "\n",
        "        temp['std_month_Close'] = datasets[dataset]['Close'].rolling(window=30).std()\n",
        "\n",
        "        datasets[dataset] = temp.copy()\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "    #Rename the columns\n",
        "    count = 0\n",
        "    for dataset in datasets:\n",
        "\n",
        "        datasets[dataset] = datasets[dataset].rename(columns={'Close':dataset+'_Close', 'Open':dataset+'_Open', \n",
        "                                                              'High':dataset+'_High', 'Low':dataset+'_Low', \n",
        "                                                              'Adj Close':dataset+'_Adj_Close', \n",
        "                                                              'Volume':dataset+'_Volume', \n",
        "                                                              'close_off_high':dataset+'_close_off_high',\n",
        "                                                              'volatility':dataset+'_volatility',\n",
        "                                                              'mean_7days_Close':dataset+'_mean_7days_Close',\n",
        "                                                              'mean_month_Close':dataset+'_mean_month_Close',\n",
        "                                                              'std_7days_Close':dataset+'_std_7days_Close',\n",
        "                                                              'std_month_Close':dataset+'_std_month_Close'})\n",
        "        \n",
        "        \n",
        "        if count == 0:\n",
        "            \n",
        "            date_col = (datasets[dataset]['Date'].reset_index()).drop(['index'], axis=1)\n",
        "            \n",
        "            \n",
        "        datasets[dataset] = ((datasets[dataset].drop(['Date'], axis=1)).reset_index()).drop(['index'], axis=1)    \n",
        "                    \n",
        "        \n",
        "\n",
        "    whole_dataset = pd.concat([datasets[dataset] for dataset in datasets], axis=1)\n",
        "    whole_dataset = pd.concat([date_col, whole_dataset], axis=1)\n",
        "    \n",
        "   \n",
        "    return whole_dataset"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXpt3VwUa4NG"
      },
      "source": [
        "Split data into training, validation and test set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jADuiFRa4NH"
      },
      "source": [
        "def split_data(perc_train_set, perc_val_set, currency_data):\n",
        "        \n",
        "        #Compute the date to split the dataset into training and validation_test set based on 'perc_train_set'\n",
        "        splt_date_train = currency_data.iloc[round(currency_data.shape[0] * perc_train_set)]['Date']\n",
        "        \n",
        "        #Split the dataset into trainning and validation_test set\n",
        "        tr_set, val_tst_set = currency_data[currency_data['Date'] < splt_date_train], \\\n",
        "                         currency_data[currency_data['Date'] >= splt_date_train]\n",
        "        \n",
        "        #Compute the date to split the val_tst_set into validation and test set based on 'perc_val_set'\n",
        "        splt_date_val = val_tst_set.iloc[round(val_tst_set.shape[0] * perc_val_set)]['Date']\n",
        "\n",
        "        #Split the val_tst_set into validation and test set        \n",
        "        val_set, tst_set = val_tst_set[val_tst_set['Date'] < splt_date_val], \\\n",
        "                            val_tst_set[val_tst_set['Date'] >= splt_date_val]\n",
        "        \n",
        "        return tr_set, val_set, tst_set, splt_date_train, splt_date_val"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6HR655Ta4NI"
      },
      "source": [
        "Normalize training, validation and test inputs and outputs with sliding window:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YMugpsTa4NI"
      },
      "source": [
        "def normalize_in_out(prd_range, wind_len, tr_set, val_set, tst_set, feats, coin_targ, crypto_list):\n",
        "    \n",
        "    all_feats = tr_set.columns #Get all features\n",
        "    feats = [crypto+\"_\"+feat for crypto in crypto_list for feat in feats] #Get the features in the appropriate format \n",
        "                                                                          #(e.g 'Close' --> 'BTC-USD_Close')\n",
        "    \n",
        "    #Normalize training inputs\n",
        "    LSTM_tr_in = []\n",
        "    for i in range(len(tr_set) - wind_len):\n",
        "        tmp_set = tr_set[i:(i + wind_len)].copy()\n",
        "        \n",
        "        for col in all_feats:\n",
        "            if col not in feats:\n",
        "                tmp_set = tmp_set.drop([col], axis=1) #Drop the feature that will not be used\n",
        "\n",
        "        for col in feats:\n",
        "            tmp_set[:][col] = tmp_set[col] / tmp_set[col].iloc[0] - 1 #Normalize the feature that will be used\n",
        "\n",
        "        LSTM_tr_in.append(tmp_set)\n",
        "    \n",
        "    #Transform from DataFrame to numpy array\n",
        "    LSTM_tr_in = [np.array(LSTM_tr_i) for LSTM_tr_i in LSTM_tr_in]\n",
        "    LSTM_tr_in = np.array(LSTM_tr_in)\n",
        "    \n",
        "    \n",
        "    #Normalize validation inputs\n",
        "    LSTM_val_in = []\n",
        "    for i in range(len(val_set) - wind_len):\n",
        "        tmp_set = val_set[i:(i + wind_len)].copy()\n",
        "        \n",
        "        for col in all_feats:\n",
        "            if col not in feats:\n",
        "                tmp_set = tmp_set.drop([col], axis=1) #Drop the feature that will not be used\n",
        "\n",
        "        for col in feats:\n",
        "            tmp_set[:][col] = tmp_set[col] / tmp_set[col].iloc[0] - 1 #Normalize the feature that will be used\n",
        "    \n",
        "        LSTM_val_in.append(tmp_set)\n",
        "        \n",
        "    #Transform from DataFrame to numpy array\n",
        "    LSTM_val_in = [np.array(LSTM_val_i) for LSTM_val_i in LSTM_val_in]\n",
        "    LSTM_val_in = np.array(LSTM_val_in)\n",
        "    \n",
        "    \n",
        "    #Normalize test inputs\n",
        "    LSTM_test_in = []\n",
        "    for i in range(len(tst_set) - wind_len):\n",
        "        tmp_set = tst_set[i:(i + wind_len)].copy() \n",
        "        \n",
        "        for col in all_feats:\n",
        "            if col not in feats:\n",
        "                \n",
        "                tmp_set = tmp_set.drop([col], axis=1) #Drop the feature that will not be used\n",
        "\n",
        "        for col in feats:\n",
        "            tmp_set[:][col] = tmp_set[col] / tmp_set[col].iloc[0] - 1 #Normalize the feature that will be used\n",
        "\n",
        "        LSTM_test_in.append(tmp_set)\n",
        "    \n",
        "    \n",
        "    #Transform from DataFrame to numpy array\n",
        "    LSTM_test_in = [np.array(LSTM_test_i) for LSTM_test_i in LSTM_test_in]\n",
        "    LSTM_test_in = np.array(LSTM_test_in)\n",
        "    \n",
        "    \n",
        "    #Normalize training outputs\n",
        "    LSTM_rangd_train_out = []\n",
        "    for i in range(wind_len, len(tr_set[coin_targ+'_Close']) - prd_range):\n",
        "        LSTM_rangd_train_out.append((tr_set[coin_targ+'_Close'][i:i+prd_range].values/tr_set[coin_targ+'_Close'][tr_set.index[0]+i-wind_len]) - 1)\n",
        "\n",
        "    LSTM_rangd_train_out = np.array(LSTM_rangd_train_out)\n",
        "    \n",
        "    \n",
        "    #Normalize validation outputs\n",
        "    LSTM_rangd_val_out = []\n",
        "    for i in range(wind_len, len(val_set[coin_targ+'_Close']) - prd_range):\n",
        "        LSTM_rangd_val_out.append((val_set[coin_targ+'_Close'][i:i+prd_range].values/val_set[coin_targ+'_Close'][val_set.index[0]+i-wind_len]) - 1)\n",
        "    \n",
        "    LSTM_rangd_val_out = np.array(LSTM_rangd_val_out)\n",
        "    \n",
        "    return LSTM_rangd_train_out, LSTM_rangd_val_out, LSTM_tr_in, LSTM_val_in, LSTM_test_in"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pzCTUd-a4NJ"
      },
      "source": [
        "Define model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dR2BT2fa4NK"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Activation, Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dropout\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def build_model(inputs, output_size, neurons, activ_func=\"linear\",\n",
        "                dropout=0.25, loss=\"mae\", optimizer=\"adam\"):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(neurons, input_shape=(inputs.shape[1], inputs.shape[2])))\n",
        "   \n",
        "    \n",
        "    model.add(Dropout(dropout))\n",
        "    \n",
        "    model.add(Dense(units=output_size))\n",
        "    model.add(Activation(activ_func))\n",
        "\n",
        "    model.compile(loss=loss, optimizer=optimizer)\n",
        "    return model"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNPyFbJ2a4NK"
      },
      "source": [
        "Bulid and train model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMxQkUdGa4NK"
      },
      "source": [
        "def build_and_train_model(epchs, btch_size, neurs, dropout, prd_range, LSTM_train_in, LSTM_rangd_train_out, LSTM_valid_in, LSTM_rangd_valid_out, shffl, verb, early_st_pat, tbd_ck):\n",
        "\n",
        "    rnged_btcoin_model = build_model(LSTM_train_in, output_size=prd_range, neurons=neurs, dropout=dropout)\n",
        "    \n",
        "    np.random.seed(202)\n",
        "    \n",
        "    callbacks = [tbd_ck ,tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=early_st_pat)]\n",
        "\n",
        "    rnged_hist = rnged_btcoin_model.fit(LSTM_train_in[:-prd_range], LSTM_rangd_train_out,\n",
        "                                      validation_data=(LSTM_valid_in[:-prd_range], LSTM_rangd_valid_out), \n",
        "                                        epochs=epchs, batch_size=btch_size, verbose=verb,  callbacks=callbacks, \n",
        "                                        shuffle=shffl)\n",
        "    \n",
        "    return rnged_btcoin_model, rnged_hist"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "CArRPWGba4NL",
        "outputId": "979d09d6-152b-4f80-929f-16a745610f94"
      },
      "source": [
        "#Get data\n",
        "crypto_list = [['BTC-USD']] #[['BTC-USD', 'ETH-USD'], ['BTC-USD']] #Define the sets of cryptocurrencies to be tested\n",
        "for cryptocurrency_list in crypto_list:\n",
        "      \n",
        "    print('Using cryptocurrencies: '+str(cryptocurrency_list))\n",
        "    data = dataset_creation(cryptocurrency_list, path)\n",
        "\n",
        "    #Split data\n",
        "    percent_train_set = 0.8\n",
        "    percent_val_set = 0.5\n",
        "\n",
        "    training_set, validation_set, test_set, split_date_train, split_date_valid = split_data(percent_train_set, \n",
        "                                                                                           percent_val_set, data)\n",
        "\n",
        "    \n",
        "    features_list = [['Close']]\n",
        "    \"\"\"[['Close'], ['Close', 'Volume'], ['Close', 'Open', 'High'],\n",
        "                     ['Close', 'close_off_high', 'volatility'],\n",
        "                     ['Close', 'mean_7days_Close', 'mean_month_Close'],\n",
        "                     ['Close', 'std_7days_Close', 'std_month_Close']]\"\"\" #Define the sets of features to be tested \n",
        "    \n",
        "    for featurs in features_list: \n",
        "    \n",
        "        print('\\tUsing the features: '+str(featurs))\n",
        "        #Create inputs and outputs for the model training, validation and testing\n",
        "        pred_range = 5\n",
        "        window_len = 10\n",
        "        features = featurs\n",
        "        coin_target = 'BTC-USD'\n",
        "\n",
        "        LSTM_ranged_training_outputs, LSTM_ranged_validation_outputs, LSTM_training_inputs, LSTM_validation_inputs, LSTM_test_inputs = normalize_in_out(\n",
        "                                                                                                      pred_range, window_len, \n",
        "                                                                                                      training_set, \n",
        "                                                                                                      validation_set, \n",
        "                                                                                                      test_set, \n",
        "                                                                                                      features, coin_target,\n",
        "                                                                                                      cryptocurrency_list)\n",
        "\n",
        "        batch_size_list = [64] #[1, 32, 64]\n",
        "        neuron_list = [20] #[20, 40, 60, 100]\n",
        "        dropout_list = [0.2] #[0.2, 0.25, 0.3, 0.4]\n",
        "\n",
        "        for bat_s, neur, drop in [(bat_s, neur, drop) for bat_s in batch_size_list for neur in neuron_list for drop in dropout_list]:\n",
        "\n",
        "            #Build and train model\n",
        "            epochs = 100\n",
        "            batch_size = bat_s\n",
        "            neurons = neur\n",
        "            dropout = drop\n",
        "            early_stop_patience = 10\n",
        "            shuffle = True\n",
        "            verbose = 0\n",
        "\n",
        "            print('\\t\\tBatch_size: '+str(batch_size)+\" Neurons: \"+str(neurons)+\" Dropout: \"+str(dropout))\n",
        "\n",
        "            log_dir = \"logs/fit/\"+ datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") \n",
        "            #+ str(cryptocurrency_list) + '/' + str(featurs) + '/' + 'Batch_size:'+str(batch_size)+\"_Neurons:\"+str(neurons)+\"_Dropout:\"+str(dropout)\n",
        "            tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "            ranged_btcoin_model, ranged_hist = build_and_train_model(epochs, batch_size, neurons, dropout, pred_range, \n",
        "                                                                     LSTM_training_inputs, LSTM_ranged_training_outputs, \n",
        "                                                                     LSTM_validation_inputs, LSTM_ranged_validation_outputs, \n",
        "                                                                     shuffle, verbose, early_stop_patience, tensorboard_callback)\n",
        "\n",
        "            print('\\t\\tBest model found in epoch ' + str(ranged_hist.epoch[-early_stop_patience]) +' with validation loss: '+str(ranged_hist.history['val_loss'][-early_stop_patience]))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cryptocurrencies: ['BTC-USD']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-1dc7fc13-790c-40e8-b5b4-86f0b44ae4a3\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-1dc7fc13-790c-40e8-b5b4-86f0b44ae4a3\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving BTC-USD.csv to BTC-USD (1).csv\n",
            "\tUsing the features: ['Close']\n",
            "\t\tBatch_size: 64 Neurons: 20 Dropout: 0.2\n",
            "\t\tBest model found in epoch 63 with validation loss: 0.041056595742702484\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ld5sBED-a4NM"
      },
      "source": [
        "#!tensorboard dev list"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXrUyvmTd9jM"
      },
      "source": [
        "#!tensorboard dev upload --logdir ./logs"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4ySqM1JeMfH"
      },
      "source": [
        "#%tensorboard --logdir logs/fit"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHjRbi8m3ZLs"
      },
      "source": [
        "# Clear any logs from previous runs\n",
        "#!tensorboard dev delete --experiment_id pgwo0Hq4SnaLBWv7EglGEg\n",
        "#%rm -rf ./logs/"
      ],
      "execution_count": 14,
      "outputs": []
    }
  ]
}