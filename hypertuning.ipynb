{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "\n",
    "#Uncomment to use it in colab and monitor to tendorboard\n",
    "\"\"\"\"%load_ext tensorboard\n",
    "\n",
    "import tensorflow as tf\n",
    "import datetime\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the path to load all locally stored csv with data from yahoo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()+\"/exper_files/datasets/\" #Define where are the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the whole dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_creation(crypto_list, pth):\n",
    "    datasets = {} #Store all datasets here\n",
    "    dates = {} #Store all minimum dates here\n",
    "    datasets_list = os.listdir(pth) #Find all dataset\n",
    "    \n",
    "    crypto_list = [elem+'.csv' for elem in crypto_list]\n",
    "\n",
    "    for dataset in datasets_list:\n",
    "        if dataset.endswith(\".csv\") and (dataset) in crypto_list:\n",
    "\n",
    "            name = dataset.split(\".\")\n",
    "            dataset_name = name[0]\n",
    "\n",
    "            datasets[dataset_name] = pd.read_csv(path + dataset) \n",
    "\n",
    "            datasets[dataset_name]['Date'] = pd.to_datetime(datasets[dataset_name]['Date'])\n",
    "            datasets[dataset_name].fillna(method='ffill', inplace=True)\n",
    "\n",
    "            #Create new columns 'close_off_high' and 'volatility' in order to make predictions more accurate:\n",
    "            kwards = {'close_off_high': lambda x: 2 * (x['High'] - x['Close']) / (x['High'] - x['Low']) - 1,\n",
    "              'volatility': lambda x: (x['High'] - x['Low']) / (x['Open'])\n",
    "              }\n",
    "\n",
    "            datasets[dataset_name] = datasets[dataset_name].assign(**kwards)\n",
    "\n",
    "            first_date = pd.to_datetime(datasets[dataset_name]['Date'][0])\n",
    "            dates[dataset_name]=first_date\n",
    "\n",
    "    max_date = max(dates.values(), key=lambda v: v)\n",
    "\n",
    "    #Drop all the data which are prior to max_date\n",
    "    for dataset in datasets:\n",
    "        datasets[dataset] = datasets[dataset][datasets[dataset]['Date'] >= max_date]\n",
    "\n",
    "\n",
    "    #Compute the average and standard deviation of 'Close' value for the last 7-days and 30-days(month): \n",
    "    for dataset in datasets:\n",
    "\n",
    "        temp = datasets[dataset].copy()\n",
    "\n",
    "        #Drop the first 30 days to be able to compute average and standard deviation of month for the rows of the table\n",
    "        temp = temp[29:]\n",
    "\n",
    "        temp['mean_7days_Close'] = datasets[dataset]['Close'].rolling(window=7).mean()\n",
    "\n",
    "        temp['mean_month_Close'] = datasets[dataset]['Close'].rolling(window=30).mean()\n",
    "\n",
    "        temp['std_7days_Close'] = datasets[dataset]['Close'].rolling(window=7).std()\n",
    "\n",
    "        temp['std_month_Close'] = datasets[dataset]['Close'].rolling(window=30).std()\n",
    "\n",
    "        datasets[dataset] = temp.copy()\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    #Rename the columns\n",
    "    count = 0\n",
    "    for dataset in datasets:\n",
    "\n",
    "        datasets[dataset] = datasets[dataset].rename(columns={'Close':dataset+'_Close', 'Open':dataset+'_Open', \n",
    "                                                              'High':dataset+'_High', 'Low':dataset+'_Low', \n",
    "                                                              'Adj Close':dataset+'_Adj_Close', \n",
    "                                                              'Volume':dataset+'_Volume', \n",
    "                                                              'close_off_high':dataset+'_close_off_high',\n",
    "                                                              'volatility':dataset+'_volatility',\n",
    "                                                              'mean_7days_Close':dataset+'_mean_7days_Close',\n",
    "                                                              'mean_month_Close':dataset+'_mean_month_Close',\n",
    "                                                              'std_7days_Close':dataset+'_std_7days_Close',\n",
    "                                                              'std_month_Close':dataset+'_std_month_Close'})\n",
    "        \n",
    "        \n",
    "        if count == 0:\n",
    "            \n",
    "            date_col = (datasets[dataset]['Date'].reset_index()).drop(['index'], axis=1)\n",
    "            \n",
    "            \n",
    "        datasets[dataset] = ((datasets[dataset].drop(['Date'], axis=1)).reset_index()).drop(['index'], axis=1)    \n",
    "                    \n",
    "        \n",
    "\n",
    "    whole_dataset = pd.concat([datasets[dataset] for dataset in datasets], axis=1)\n",
    "    whole_dataset = pd.concat([date_col, whole_dataset], axis=1)\n",
    "    \n",
    "   \n",
    "    return whole_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into training, validation and test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(perc_train_set, perc_val_set, currency_data):\n",
    "        \n",
    "        #Compute the date to split the dataset into training and validation_test set based on 'perc_train_set'\n",
    "        splt_date_train = currency_data.iloc[round(currency_data.shape[0] * perc_train_set)]['Date']\n",
    "        \n",
    "        #Split the dataset into trainning and validation_test set\n",
    "        tr_set, val_tst_set = currency_data[currency_data['Date'] < splt_date_train], \\\n",
    "                         currency_data[currency_data['Date'] >= splt_date_train]\n",
    "        \n",
    "        #Compute the date to split the val_tst_set into validation and test set based on 'perc_val_set'\n",
    "        splt_date_val = val_tst_set.iloc[round(val_tst_set.shape[0] * perc_val_set)]['Date']\n",
    "\n",
    "        #Split the val_tst_set into validation and test set        \n",
    "        val_set, tst_set = val_tst_set[val_tst_set['Date'] < splt_date_val], \\\n",
    "                            val_tst_set[val_tst_set['Date'] >= splt_date_val]\n",
    "        \n",
    "        return tr_set, val_set, tst_set, splt_date_train, splt_date_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize training, validation and test inputs and outputs with sliding window:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_in_out(prd_range, wind_len, tr_set, val_set, tst_set, feats, coin_targ, crypto_list):\n",
    "    \n",
    "    all_feats = tr_set.columns #Get all features\n",
    "    feats = [crypto+\"_\"+feat for crypto in crypto_list for feat in feats] #Get the features in the appropriate format \n",
    "                                                                          #(e.g 'Close' --> 'BTC-USD_Close')\n",
    "    \n",
    "    #Normalize training inputs\n",
    "    LSTM_tr_in = []\n",
    "    for i in range(len(tr_set) - wind_len):\n",
    "        tmp_set = tr_set[i:(i + wind_len)].copy()\n",
    "        \n",
    "        for col in all_feats:\n",
    "            if col not in feats:\n",
    "                tmp_set = tmp_set.drop([col], axis=1) #Drop the feature that will not be used\n",
    "\n",
    "        for col in feats:\n",
    "            tmp_set[:][col] = tmp_set[col] / tmp_set[col].iloc[0] - 1 #Normalize the feature that will be used\n",
    "\n",
    "        LSTM_tr_in.append(tmp_set)\n",
    "    \n",
    "    #Transform from DataFrame to numpy array\n",
    "    LSTM_tr_in = [np.array(LSTM_tr_i) for LSTM_tr_i in LSTM_tr_in]\n",
    "    LSTM_tr_in = np.array(LSTM_tr_in)\n",
    "    \n",
    "    \n",
    "    #Normalize validation inputs\n",
    "    LSTM_val_in = []\n",
    "    for i in range(len(val_set) - wind_len):\n",
    "        tmp_set = val_set[i:(i + wind_len)].copy()\n",
    "        \n",
    "        for col in all_feats:\n",
    "            if col not in feats:\n",
    "                tmp_set = tmp_set.drop([col], axis=1) #Drop the feature that will not be used\n",
    "\n",
    "        for col in feats:\n",
    "            tmp_set[:][col] = tmp_set[col] / tmp_set[col].iloc[0] - 1 #Normalize the feature that will be used\n",
    "    \n",
    "        LSTM_val_in.append(tmp_set)\n",
    "        \n",
    "    #Transform from DataFrame to numpy array\n",
    "    LSTM_val_in = [np.array(LSTM_val_i) for LSTM_val_i in LSTM_val_in]\n",
    "    LSTM_val_in = np.array(LSTM_val_in)\n",
    "    \n",
    "    \n",
    "    #Normalize test inputs\n",
    "    LSTM_test_in = []\n",
    "    for i in range(len(tst_set) - wind_len):\n",
    "        tmp_set = tst_set[i:(i + wind_len)].copy() \n",
    "        \n",
    "        for col in all_feats:\n",
    "            if col not in feats:\n",
    "                \n",
    "                tmp_set = tmp_set.drop([col], axis=1) #Drop the feature that will not be used\n",
    "\n",
    "        for col in feats:\n",
    "            tmp_set[:][col] = tmp_set[col] / tmp_set[col].iloc[0] - 1 #Normalize the feature that will be used\n",
    "\n",
    "        LSTM_test_in.append(tmp_set)\n",
    "    \n",
    "    \n",
    "    #Transform from DataFrame to numpy array\n",
    "    LSTM_test_in = [np.array(LSTM_test_i) for LSTM_test_i in LSTM_test_in]\n",
    "    LSTM_test_in = np.array(LSTM_test_in)\n",
    "    \n",
    "    \n",
    "    #Normalize training outputs\n",
    "    LSTM_rangd_train_out = []\n",
    "    for i in range(wind_len, len(tr_set[coin_targ+'_Close']) - prd_range):\n",
    "        LSTM_rangd_train_out.append((tr_set[coin_targ+'_Close'][i:i+prd_range].values/tr_set[coin_targ+'_Close'][tr_set.index[0]+i-wind_len]) - 1)\n",
    "\n",
    "    LSTM_rangd_train_out = np.array(LSTM_rangd_train_out)\n",
    "    \n",
    "    \n",
    "    #Normalize validation outputs\n",
    "    LSTM_rangd_val_out = []\n",
    "    for i in range(wind_len, len(val_set[coin_targ+'_Close']) - prd_range):\n",
    "        LSTM_rangd_val_out.append((val_set[coin_targ+'_Close'][i:i+prd_range].values/val_set[coin_targ+'_Close'][val_set.index[0]+i-wind_len]) - 1)\n",
    "    \n",
    "    LSTM_rangd_val_out = np.array(LSTM_rangd_val_out)\n",
    "    \n",
    "    return LSTM_rangd_train_out, LSTM_rangd_val_out, LSTM_tr_in, LSTM_val_in, LSTM_test_in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def build_model(inputs, output_size, neurons, activ_func=\"linear\",\n",
    "                dropout=0.25, loss=\"mae\", optimizer=\"adam\"):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(neurons, input_shape=(inputs.shape[1], inputs.shape[2])))\n",
    "   \n",
    "    \n",
    "    model.add(Dropout(dropout))\n",
    "    \n",
    "    model.add(Dense(units=output_size))\n",
    "    model.add(Activation(activ_func))\n",
    "\n",
    "    model.compile(loss=loss, optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bulid and train model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_train_model(epchs, btch_size, neurs, dropout, prd_range, LSTM_train_in, LSTM_rangd_train_out, LSTM_valid_in, LSTM_rangd_valid_out, shffl, verb, early_st_pat\"\"\", tbd_ck\"\"\"):\n",
    "\n",
    "    rnged_btcoin_model = build_model(LSTM_train_in, output_size=prd_range, neurons=neurs, dropout=dropout)\n",
    "    \n",
    "    np.random.seed(202)\n",
    "    \n",
    "    callbacks = [\"\"\"tbd_ck, \"\"\"tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=early_st_pat)]\n",
    "\n",
    "    rnged_hist = rnged_btcoin_model.fit(LSTM_train_in[:-prd_range], LSTM_rangd_train_out,\n",
    "                                      validation_data=(LSTM_valid_in[:-prd_range], LSTM_rangd_valid_out), \n",
    "                                        epochs=epchs, batch_size=btch_size, verbose=verb,  callback=callbacks, \n",
    "                                        shuffle=shffl)\n",
    "    \n",
    "    return rnged_btcoin_model, rnged_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cryptocurrencies: ['BTC-USD', 'ETH-USD']\n",
      "\tUsing the features: ['Close']\n",
      "\t\tBatch_size: 1 Neurons: 20 Dropout: 0.2\n",
      "\t\tBest model found in epoch 7 with validation loss: 0.03555181995034218\n",
      "\t\tBatch_size: 1 Neurons: 20 Dropout: 0.25\n",
      "\t\tBest model found in epoch 7 with validation loss: 0.03686963766813278\n",
      "\t\tBatch_size: 1 Neurons: 20 Dropout: 0.3\n",
      "\t\tBest model found in epoch 8 with validation loss: 0.03547072410583496\n",
      "\t\tBatch_size: 1 Neurons: 20 Dropout: 0.4\n",
      "\t\tBest model found in epoch 11 with validation loss: 0.035500068217515945\n",
      "\t\tBatch_size: 1 Neurons: 40 Dropout: 0.2\n",
      "\t\tBest model found in epoch 5 with validation loss: 0.03589186072349548\n",
      "\t\tBatch_size: 1 Neurons: 40 Dropout: 0.25\n",
      "\t\tBest model found in epoch 4 with validation loss: 0.03716958314180374\n",
      "\t\tBatch_size: 1 Neurons: 40 Dropout: 0.3\n",
      "\t\tBest model found in epoch 14 with validation loss: 0.03564949333667755\n",
      "\t\tBatch_size: 1 Neurons: 40 Dropout: 0.4\n",
      "\t\tBest model found in epoch 6 with validation loss: 0.036535829305648804\n",
      "\t\tBatch_size: 1 Neurons: 60 Dropout: 0.2\n",
      "\t\tBest model found in epoch 4 with validation loss: 0.036433588713407516\n",
      "\t\tBatch_size: 1 Neurons: 60 Dropout: 0.25\n",
      "\t\tBest model found in epoch 32 with validation loss: 0.03639861196279526\n",
      "\t\tBatch_size: 1 Neurons: 60 Dropout: 0.3\n",
      "\t\tBest model found in epoch 5 with validation loss: 0.035842813551425934\n",
      "\t\tBatch_size: 1 Neurons: 60 Dropout: 0.4\n",
      "\t\tBest model found in epoch 25 with validation loss: 0.03694735839962959\n",
      "\t\tBatch_size: 1 Neurons: 100 Dropout: 0.2\n",
      "\t\tBest model found in epoch 12 with validation loss: 0.03560467064380646\n",
      "\t\tBatch_size: 1 Neurons: 100 Dropout: 0.25\n",
      "\t\tBest model found in epoch 3 with validation loss: 0.03684232383966446\n",
      "\t\tBatch_size: 1 Neurons: 100 Dropout: 0.3\n",
      "\t\tBest model found in epoch 13 with validation loss: 0.0361664816737175\n",
      "\t\tBatch_size: 1 Neurons: 100 Dropout: 0.4\n",
      "\t\tBest model found in epoch 6 with validation loss: 0.03579651191830635\n",
      "\t\tBatch_size: 32 Neurons: 20 Dropout: 0.2\n",
      "\t\tBest model found in epoch 36 with validation loss: 0.03522971644997597\n",
      "\t\tBatch_size: 32 Neurons: 20 Dropout: 0.25\n",
      "\t\tBest model found in epoch 54 with validation loss: 0.03538581356406212\n",
      "\t\tBatch_size: 32 Neurons: 20 Dropout: 0.3\n",
      "\t\tBest model found in epoch 38 with validation loss: 0.03538695350289345\n",
      "\t\tBatch_size: 32 Neurons: 20 Dropout: 0.4\n",
      "\t\tBest model found in epoch 56 with validation loss: 0.03532687574625015\n",
      "\t\tBatch_size: 32 Neurons: 40 Dropout: 0.2\n",
      "\t\tBest model found in epoch 32 with validation loss: 0.03532632067799568\n",
      "\t\tBatch_size: 32 Neurons: 40 Dropout: 0.25\n",
      "\t\tBest model found in epoch 26 with validation loss: 0.03541207313537598\n",
      "\t\tBatch_size: 32 Neurons: 40 Dropout: 0.3\n",
      "\t\tBest model found in epoch 29 with validation loss: 0.03543572127819061\n",
      "\t\tBatch_size: 32 Neurons: 40 Dropout: 0.4\n",
      "\t\tBest model found in epoch 25 with validation loss: 0.0354088731110096\n",
      "\t\tBatch_size: 32 Neurons: 60 Dropout: 0.2\n",
      "\t\tBest model found in epoch 26 with validation loss: 0.03528647497296333\n",
      "\t\tBatch_size: 32 Neurons: 60 Dropout: 0.25\n",
      "\t\tBest model found in epoch 24 with validation loss: 0.035272542387247086\n",
      "\t\tBatch_size: 32 Neurons: 60 Dropout: 0.3\n",
      "\t\tBest model found in epoch 25 with validation loss: 0.03523772582411766\n",
      "\t\tBatch_size: 32 Neurons: 60 Dropout: 0.4\n",
      "\t\tBest model found in epoch 23 with validation loss: 0.03539832681417465\n",
      "\t\tBatch_size: 32 Neurons: 100 Dropout: 0.2\n",
      "\t\tBest model found in epoch 17 with validation loss: 0.035385701805353165\n",
      "\t\tBatch_size: 32 Neurons: 100 Dropout: 0.25\n",
      "\t\tBest model found in epoch 27 with validation loss: 0.03532085195183754\n",
      "\t\tBatch_size: 32 Neurons: 100 Dropout: 0.3\n",
      "\t\tBest model found in epoch 19 with validation loss: 0.03639894351363182\n",
      "\t\tBatch_size: 32 Neurons: 100 Dropout: 0.4\n",
      "\t\tBest model found in epoch 23 with validation loss: 0.035346679389476776\n",
      "\t\tBatch_size: 64 Neurons: 20 Dropout: 0.2\n",
      "\t\tBest model found in epoch 55 with validation loss: 0.03549722954630852\n",
      "\t\tBatch_size: 64 Neurons: 20 Dropout: 0.25\n",
      "\t\tBest model found in epoch 69 with validation loss: 0.03521815687417984\n",
      "\t\tBatch_size: 64 Neurons: 20 Dropout: 0.3\n",
      "\t\tBest model found in epoch 50 with validation loss: 0.03523224592208862\n",
      "\t\tBatch_size: 64 Neurons: 20 Dropout: 0.4\n",
      "\t\tBest model found in epoch 72 with validation loss: 0.03516719117760658\n",
      "\t\tBatch_size: 64 Neurons: 40 Dropout: 0.2\n",
      "\t\tBest model found in epoch 43 with validation loss: 0.0353725329041481\n",
      "\t\tBatch_size: 64 Neurons: 40 Dropout: 0.25\n",
      "\t\tBest model found in epoch 41 with validation loss: 0.03529557213187218\n",
      "\t\tBatch_size: 64 Neurons: 40 Dropout: 0.3\n",
      "\t\tBest model found in epoch 45 with validation loss: 0.03552817180752754\n",
      "\t\tBatch_size: 64 Neurons: 40 Dropout: 0.4\n",
      "\t\tBest model found in epoch 48 with validation loss: 0.03530309721827507\n",
      "\t\tBatch_size: 64 Neurons: 60 Dropout: 0.2\n",
      "\t\tBest model found in epoch 39 with validation loss: 0.03522137925028801\n",
      "\t\tBatch_size: 64 Neurons: 60 Dropout: 0.25\n",
      "\t\tBest model found in epoch 32 with validation loss: 0.03558265417814255\n",
      "\t\tBatch_size: 64 Neurons: 60 Dropout: 0.3\n",
      "\t\tBest model found in epoch 42 with validation loss: 0.03551207482814789\n",
      "\t\tBatch_size: 64 Neurons: 60 Dropout: 0.4\n",
      "\t\tBest model found in epoch 45 with validation loss: 0.03516555204987526\n",
      "\t\tBatch_size: 64 Neurons: 100 Dropout: 0.2\n",
      "\t\tBest model found in epoch 25 with validation loss: 0.03529363498091698\n",
      "\t\tBatch_size: 64 Neurons: 100 Dropout: 0.25\n",
      "\t\tBest model found in epoch 33 with validation loss: 0.035271428525447845\n",
      "\t\tBatch_size: 64 Neurons: 100 Dropout: 0.3\n",
      "\t\tBest model found in epoch 29 with validation loss: 0.03553701192140579\n",
      "\t\tBatch_size: 64 Neurons: 100 Dropout: 0.4\n",
      "\t\tBest model found in epoch 34 with validation loss: 0.03522062674164772\n",
      "\tUsing the features: ['Close', 'Volume']\n",
      "\t\tBatch_size: 1 Neurons: 20 Dropout: 0.2\n",
      "\t\tBest model found in epoch 6 with validation loss: 0.035942982882261276\n",
      "\t\tBatch_size: 1 Neurons: 20 Dropout: 0.25\n",
      "\t\tBest model found in epoch 15 with validation loss: 0.03610923886299133\n",
      "\t\tBatch_size: 1 Neurons: 20 Dropout: 0.3\n",
      "\t\tBest model found in epoch 5 with validation loss: 0.03691987320780754\n",
      "\t\tBatch_size: 1 Neurons: 20 Dropout: 0.4\n",
      "\t\tBest model found in epoch 8 with validation loss: 0.036201708018779755\n",
      "\t\tBatch_size: 1 Neurons: 40 Dropout: 0.2\n",
      "\t\tBest model found in epoch 7 with validation loss: 0.03648078441619873\n",
      "\t\tBatch_size: 1 Neurons: 40 Dropout: 0.25\n",
      "\t\tBest model found in epoch 38 with validation loss: 0.03611961752176285\n",
      "\t\tBatch_size: 1 Neurons: 40 Dropout: 0.3\n",
      "\t\tBest model found in epoch 5 with validation loss: 0.03709591552615166\n",
      "\t\tBatch_size: 1 Neurons: 40 Dropout: 0.4\n",
      "\t\tBest model found in epoch 12 with validation loss: 0.037524525076150894\n",
      "\t\tBatch_size: 1 Neurons: 60 Dropout: 0.2\n",
      "\t\tBest model found in epoch 14 with validation loss: 0.036109596490859985\n",
      "\t\tBatch_size: 1 Neurons: 60 Dropout: 0.25\n",
      "\t\tBest model found in epoch 12 with validation loss: 0.03600979968905449\n",
      "\t\tBatch_size: 1 Neurons: 60 Dropout: 0.3\n",
      "\t\tBest model found in epoch 10 with validation loss: 0.03686262294650078\n",
      "\t\tBatch_size: 1 Neurons: 60 Dropout: 0.4\n",
      "\t\tBest model found in epoch 16 with validation loss: 0.0364917628467083\n",
      "\t\tBatch_size: 1 Neurons: 100 Dropout: 0.2\n",
      "\t\tBest model found in epoch 5 with validation loss: 0.03636937960982323\n",
      "\t\tBatch_size: 1 Neurons: 100 Dropout: 0.25\n",
      "\t\tBest model found in epoch 2 with validation loss: 0.03902014344930649\n",
      "\t\tBatch_size: 1 Neurons: 100 Dropout: 0.3\n",
      "\t\tBest model found in epoch 23 with validation loss: 0.03675535321235657\n",
      "\t\tBatch_size: 1 Neurons: 100 Dropout: 0.4\n",
      "\t\tBest model found in epoch 5 with validation loss: 0.03876315429806709\n",
      "\t\tBatch_size: 32 Neurons: 20 Dropout: 0.2\n",
      "\t\tBest model found in epoch 46 with validation loss: 0.03553057089447975\n",
      "\t\tBatch_size: 32 Neurons: 20 Dropout: 0.25\n",
      "\t\tBest model found in epoch 40 with validation loss: 0.03566701337695122\n",
      "\t\tBatch_size: 32 Neurons: 20 Dropout: 0.3\n",
      "\t\tBest model found in epoch 43 with validation loss: 0.03545336052775383\n",
      "\t\tBatch_size: 32 Neurons: 20 Dropout: 0.4\n",
      "\t\tBest model found in epoch 58 with validation loss: 0.035567741841077805\n",
      "\t\tBatch_size: 32 Neurons: 40 Dropout: 0.2\n",
      "\t\tBest model found in epoch 33 with validation loss: 0.03536301851272583\n",
      "\t\tBatch_size: 32 Neurons: 40 Dropout: 0.25\n",
      "\t\tBest model found in epoch 43 with validation loss: 0.035652581602334976\n",
      "\t\tBatch_size: 32 Neurons: 40 Dropout: 0.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tBest model found in epoch 45 with validation loss: 0.03539042919874191\n",
      "\t\tBatch_size: 32 Neurons: 40 Dropout: 0.4\n",
      "\t\tBest model found in epoch 41 with validation loss: 0.03535907715559006\n",
      "\t\tBatch_size: 32 Neurons: 60 Dropout: 0.2\n",
      "\t\tBest model found in epoch 36 with validation loss: 0.035388536751270294\n",
      "\t\tBatch_size: 32 Neurons: 60 Dropout: 0.25\n",
      "\t\tBest model found in epoch 30 with validation loss: 0.0356292799115181\n",
      "\t\tBatch_size: 32 Neurons: 60 Dropout: 0.3\n",
      "\t\tBest model found in epoch 30 with validation loss: 0.03562966734170914\n",
      "\t\tBatch_size: 32 Neurons: 60 Dropout: 0.4\n",
      "\t\tBest model found in epoch 45 with validation loss: 0.03570662811398506\n",
      "\t\tBatch_size: 32 Neurons: 100 Dropout: 0.2\n",
      "\t\tBest model found in epoch 28 with validation loss: 0.035730741918087006\n",
      "\t\tBatch_size: 32 Neurons: 100 Dropout: 0.25\n",
      "\t\tBest model found in epoch 28 with validation loss: 0.03568040207028389\n",
      "\t\tBatch_size: 32 Neurons: 100 Dropout: 0.3\n",
      "\t\tBest model found in epoch 41 with validation loss: 0.0355195589363575\n",
      "\t\tBatch_size: 32 Neurons: 100 Dropout: 0.4\n",
      "\t\tBest model found in epoch 30 with validation loss: 0.03556931018829346\n",
      "\t\tBatch_size: 64 Neurons: 20 Dropout: 0.2\n",
      "\t\tBest model found in epoch 68 with validation loss: 0.03537352383136749\n",
      "\t\tBatch_size: 64 Neurons: 20 Dropout: 0.25\n",
      "\t\tBest model found in epoch 76 with validation loss: 0.0353841669857502\n",
      "\t\tBatch_size: 64 Neurons: 20 Dropout: 0.3\n",
      "\t\tBest model found in epoch 65 with validation loss: 0.03537435084581375\n",
      "\t\tBatch_size: 64 Neurons: 20 Dropout: 0.4\n",
      "\t\tBest model found in epoch 89 with validation loss: 0.03537154942750931\n",
      "\t\tBatch_size: 64 Neurons: 40 Dropout: 0.2\n",
      "\t\tBest model found in epoch 51 with validation loss: 0.035300061106681824\n",
      "\t\tBatch_size: 64 Neurons: 40 Dropout: 0.25\n",
      "\t\tBest model found in epoch 53 with validation loss: 0.03552481159567833\n",
      "\t\tBatch_size: 64 Neurons: 40 Dropout: 0.3\n",
      "\t\tBest model found in epoch 54 with validation loss: 0.035575903952121735\n",
      "\t\tBatch_size: 64 Neurons: 40 Dropout: 0.4\n",
      "\t\tBest model found in epoch 56 with validation loss: 0.035462308675050735\n",
      "\t\tBatch_size: 64 Neurons: 60 Dropout: 0.2\n",
      "\t\tBest model found in epoch 45 with validation loss: 0.035739120095968246\n",
      "\t\tBatch_size: 64 Neurons: 60 Dropout: 0.25\n",
      "\t\tBest model found in epoch 37 with validation loss: 0.03557717800140381\n",
      "\t\tBatch_size: 64 Neurons: 60 Dropout: 0.3\n",
      "\t\tBest model found in epoch 53 with validation loss: 0.03540594503283501\n",
      "\t\tBatch_size: 64 Neurons: 60 Dropout: 0.4\n",
      "\t\tBest model found in epoch 49 with validation loss: 0.03544106334447861\n",
      "\t\tBatch_size: 64 Neurons: 100 Dropout: 0.2\n",
      "\t\tBest model found in epoch 37 with validation loss: 0.03552330657839775\n",
      "\t\tBatch_size: 64 Neurons: 100 Dropout: 0.25\n",
      "\t\tBest model found in epoch 48 with validation loss: 0.035451412200927734\n",
      "\t\tBatch_size: 64 Neurons: 100 Dropout: 0.3\n",
      "\t\tBest model found in epoch 43 with validation loss: 0.03544280305504799\n",
      "\t\tBatch_size: 64 Neurons: 100 Dropout: 0.4\n",
      "\t\tBest model found in epoch 44 with validation loss: 0.03550920635461807\n",
      "\tUsing the features: ['Close', 'Open', 'High']\n",
      "\t\tBatch_size: 1 Neurons: 20 Dropout: 0.2\n",
      "\t\tBest model found in epoch 5 with validation loss: 0.03573745861649513\n",
      "\t\tBatch_size: 1 Neurons: 20 Dropout: 0.25\n",
      "\t\tBest model found in epoch 5 with validation loss: 0.036059800535440445\n",
      "\t\tBatch_size: 1 Neurons: 20 Dropout: 0.3\n",
      "\t\tBest model found in epoch 13 with validation loss: 0.035771485418081284\n",
      "\t\tBatch_size: 1 Neurons: 20 Dropout: 0.4\n",
      "\t\tBest model found in epoch 21 with validation loss: 0.03595401719212532\n",
      "\t\tBatch_size: 1 Neurons: 40 Dropout: 0.2\n",
      "\t\tBest model found in epoch 21 with validation loss: 0.037153638899326324\n",
      "\t\tBatch_size: 1 Neurons: 40 Dropout: 0.25\n",
      "\t\tBest model found in epoch 4 with validation loss: 0.03622649610042572\n",
      "\t\tBatch_size: 1 Neurons: 40 Dropout: 0.3\n",
      "\t\tBest model found in epoch 6 with validation loss: 0.036728162318468094\n",
      "\t\tBatch_size: 1 Neurons: 40 Dropout: 0.4\n"
     ]
    }
   ],
   "source": [
    "#Get data\n",
    "crypto_list = [['BTC-USD', 'ETH-USD'], ['BTC-USD']] #Define the sets of cryptocurrencies to be tested\n",
    "for cryptocurrency_list in crypto_list:\n",
    "      \n",
    "    print('Using cryptocurrencies: '+str(cryptocurrency_list))\n",
    "    data = dataset_creation(cryptocurrency_list, path)\n",
    "\n",
    "    #Split data\n",
    "    percent_train_set = 0.8\n",
    "    percent_val_set = 0.5\n",
    "\n",
    "    training_set, validation_set, test_set, split_date_train, split_date_valid = split_data(percent_train_set, \n",
    "                                                                                           percent_val_set, data)\n",
    "\n",
    "    \n",
    "    features_list = [['Close'], ['Close', 'Volume'], ['Close', 'Open', 'High'],\n",
    "                     ['Close', 'close_off_high', 'volatility'],\n",
    "                     ['Close', 'mean_7days_Close', 'mean_month_Close'],\n",
    "                     ['Close', 'std_7days_Close', 'std_month_Close']] #Define the sets of features to be tested \n",
    "    \n",
    "    for featurs in features_list: \n",
    "    \n",
    "        print('\\tUsing the features: '+str(featurs))\n",
    "        #Create inputs and outputs for the model training, validation and testing\n",
    "        pred_range = 5\n",
    "        window_len = 10\n",
    "        features = featurs\n",
    "        coin_target = 'BTC-USD'\n",
    "\n",
    "        LSTM_ranged_training_outputs, LSTM_ranged_validation_outputs, LSTM_training_inputs, LSTM_validation_inputs, LSTM_test_inputs = normalize_in_out(\n",
    "                                                                                                      pred_range, window_len, \n",
    "                                                                                                      training_set, \n",
    "                                                                                                      validation_set, \n",
    "                                                                                                      test_set, \n",
    "                                                                                                      features, coin_target,\n",
    "                                                                                                      cryptocurrency_list)\n",
    "\n",
    "        batch_size_list = [1, 32, 64]\n",
    "        neuron_list = [20, 40, 60, 100]\n",
    "        dropout_list = [0.2, 0.25, 0.3, 0.4]\n",
    "\n",
    "        for bat_s, neur, drop in [(bat_s, neur, drop) for bat_s in batch_size_list for neur in neuron_list for drop in dropout_list]:\n",
    "\n",
    "            #Build and train model\n",
    "            epochs = 100\n",
    "            batch_size = bat_s\n",
    "            neurons = neur\n",
    "            dropout = drop\n",
    "            early_stop_patience = 10\n",
    "            shuffle = True\n",
    "            verbose = 0\n",
    "\n",
    "            print('\\t\\tBatch_size: '+str(batch_size)+\" Neurons: \"+str(neurons)+\" Dropout: \"+str(dropout))\n",
    "            \n",
    "            #Unocomment to monitor in tensorboard\n",
    "            \"\"\"log_dir = \"logs/fit/\" + str(cryptocurrency_list) + '/' + str(featurs) + '/' + 'Batch_size:'+str(batch_size)+\"_Neurons:\"+str(neurons)+\"_Dropout:\"+str(dropout)\n",
    "            tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "            \"\"\"\n",
    "            \n",
    "            ranged_btcoin_model, ranged_hist = build_and_train_model(epochs, batch_size, neurons, dropout, pred_range, \n",
    "                                                                     LSTM_training_inputs, LSTM_ranged_training_outputs, \n",
    "                                                                     LSTM_validation_inputs, LSTM_ranged_validation_outputs, \n",
    "                                                                     shuffle, verbose, early_stop_patience\", tensorboard_callback\")\n",
    "\n",
    "            print('\\t\\tBest model found in epoch ' + str(ranged_hist.epoch[-early_stop_patience]) +' with validation loss: '+str(ranged_hist.history['val_loss'][-early_stop_patience]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uncomment to monitor in tensorboard\n",
    "#%tensorboard --logdir log_dir"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
